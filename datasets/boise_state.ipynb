{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the following params correctly\n",
    "* **readme_txt_path**  : Path to The **README.txt** file under **Boise State Bangla Handwriting Dataset 20200228**folder \n",
    "* **save_path**  : Path to save the processed data\n",
    "* **split_test** : %of test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_txt_path=\"/media/ansary/DriveData/Work/bengalAI/datasets/\"+\\\n",
    "                \"__raw__/Boise State Bangla Handwriting Dataset 20200228/README.txt\"\n",
    "\n",
    "save_path=\"/media/ansary/DriveData/Work/bengalAI/datasets/Recognition/\"\n",
    "split_test=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#----------------------\n",
    "# imports\n",
    "#----------------------\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from coreLib.utils import stripPads,LOG_INFO,create_dir\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import random\n",
    "random.seed(42)\n",
    "base_path=os.path.dirname(readme_txt_path)\n",
    "LOG_INFO(base_path)\n",
    "assert len(os.listdir(base_path))==5,\"WRONG PATH FOR README.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(_dir,coords,fmt):\n",
    "    '''\n",
    "        extracts information from boise-state annotations\n",
    "    '''\n",
    "    img_paths=[img_path for img_path in glob(os.path.join(_dir,f\"*.{fmt}\"))]\n",
    "    liness=[]\n",
    "    words=[]\n",
    "    comps=[]\n",
    "    chars=[]\n",
    "    xmins=[]\n",
    "    ymins=[]\n",
    "    xmaxs=[]\n",
    "    ymaxs=[]\n",
    "    _paths=[]\n",
    "    # images\n",
    "    for img_path in tqdm(img_paths):\n",
    "        base=img_path.split(\".\")[0]\n",
    "        # text path\n",
    "        _iden=os.path.basename(img_path).split(\".\")[0]\n",
    "        text_path=os.path.join(_dir,coords,f\"{_iden}.txt\")\n",
    "        with open(text_path,\"r\") as tf:\n",
    "            lines=tf.readlines()\n",
    "        for line in lines:\n",
    "            parts=line.split()\n",
    "            if len(parts)>4:\n",
    "                line_num=parts[0].replace(\"\\ufeff\",\"\")\n",
    "                word_num=parts[1]\n",
    "                label=parts[2]\n",
    "                data=parts[3]\n",
    "                x,y,w,h=[int(i) for i in parts[-1].split(\",\")]\n",
    "                liness.append(line_num)\n",
    "                words.append(word_num)\n",
    "                chars.append(label)\n",
    "                xmins.append(x)\n",
    "                ymins.append(y)\n",
    "                xmaxs.append(x+w)\n",
    "                ymaxs.append(y+h)\n",
    "                _paths.append(img_path)\n",
    "                comps.append(data)\n",
    "    df=pd.DataFrame({\"line\":liness,\n",
    "                     \"word\":words,\n",
    "                     \"char\":chars,\n",
    "                     \"comp\":comps,\n",
    "                     \"xmin\":xmins,\n",
    "                     \"ymin\":ymins,\n",
    "                     \"xmax\":xmaxs,\n",
    "                     \"ymax\":ymaxs,\n",
    "                     \"image\":_paths})\n",
    "    return df\n",
    "\n",
    "def check_missing(_dir,coords,fmt):\n",
    "    '''\n",
    "        checks for missing data\n",
    "    '''\n",
    "    img_paths=[img_path for img_path in glob(os.path.join(_dir,f\"*.{fmt}\"))]\n",
    "    txt_paths=[txt_path for txt_path in glob(os.path.join(_dir,coords,\"*.txt\"))]\n",
    "    # error check\n",
    "    for img_path in tqdm(img_paths):\n",
    "        if \"jpg\" in img_path:\n",
    "            _iden=os.path.basename(img_path).split(\".\")[0]\n",
    "            txt_path=os.path.join(_dir,coords,f\"{_iden}.txt\")\n",
    "            if not os.path.exists(txt_path):\n",
    "                print(img_path)\n",
    "                for txt in txt_paths:\n",
    "                    if _iden in txt:\n",
    "                        print(txt)\n",
    "                        niden=os.path.basename(txt).split('.')[0]\n",
    "                        print(f\"RENAME:{_iden} to {niden}\")\n",
    "                        os.rename(os.path.join(_dir,f\"{_iden}.{fmt}\"),\n",
    "                                  os.path.join(_dir,f\"{niden}.{fmt}\"))\n",
    "                        \n",
    "                        \n",
    "def removeShadow(img):\n",
    "    rgb_planes = cv2.split(img)\n",
    "\n",
    "    result_planes = []\n",
    "    result_norm_planes = []\n",
    "    for plane in rgb_planes:\n",
    "        dilated_img = cv2.dilate(plane, np.ones((7,7), np.uint8))\n",
    "        bg_img = cv2.medianBlur(dilated_img, 21)\n",
    "        diff_img = 255 - cv2.absdiff(plane, bg_img)\n",
    "        norm_img = cv2.normalize(diff_img,None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "        result_planes.append(diff_img)\n",
    "        result_norm_planes.append(norm_img)\n",
    "\n",
    "    result = cv2.merge(result_planes)\n",
    "    result_norm = cv2.merge(result_norm_planes)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(base_path)\n",
    "dfs=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir=os.path.join(base_path,'1. Camera','1. Essay')\n",
    "coords='Character Coordinates_a'\n",
    "fmt=\"jpg\"\n",
    "check_missing(_dir,coords,fmt)\n",
    "dfs.append(extract_info(_dir,coords,fmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir=os.path.join(base_path,'2. Scan','1. Essay')\n",
    "coords='Character Coordinates_a'\n",
    "fmt=\"tif\"\n",
    "check_missing(_dir,coords,fmt)\n",
    "dfs.append(extract_info(_dir,coords,fmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conjunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir=os.path.join(base_path,'3. Conjunct')\n",
    "coords='Character Coordinates'\n",
    "fmt=\"tif\"\n",
    "check_missing(_dir,coords,fmt)\n",
    "dfs.append(extract_info(_dir,coords,fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(dfs,ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=create_dir(save_path,\"bs\")\n",
    "save_path=create_dir(main_path,\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"filename\",\"word\",\"graphemes\",\"mode\"\n",
    "filename=[]\n",
    "graphemes=[]\n",
    "mode=[]\n",
    "iden=0\n",
    "for img_path in tqdm(df.image.unique()):\n",
    "    idf=df.loc[df.image==img_path]\n",
    "    #-------------\n",
    "    # image\n",
    "    #-------------\n",
    "    img=cv2.imread(img_path)\n",
    "    \n",
    "    # charmap\n",
    "    cimg=removeShadow(img)\n",
    "    cimg=cv2.cvtColor(cimg, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(cimg,(5,5),0)\n",
    "    _,img = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    for line in idf.line.unique():\n",
    "        linedf=idf.loc[idf.line==line]\n",
    "        for word in linedf.word.unique():\n",
    "            wdf=linedf.loc[linedf.word==word]\n",
    "            # word\n",
    "            xmin=int(min(wdf.xmin.tolist()))\n",
    "            xmax=int(max(wdf.xmax.tolist()))\n",
    "\n",
    "            ymin=int(min(wdf.ymin.tolist()))\n",
    "            ymax=int(max(wdf.ymax.tolist()))\n",
    "\n",
    "            data=img[ymin:ymax,xmin:xmax]\n",
    "            data=stripPads(data,255)\n",
    "            \n",
    "            fname=f\"{iden}.png\"\n",
    "            filename.append(fname)\n",
    "            cv2.imwrite(os.path.join(save_path,fname),data)\n",
    "            graphemes.append(wdf.comp.tolist())\n",
    "            mode.append(img_path.replace(base_path,\"\"))\n",
    "            iden+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({\"filename\":filename,\"graphemes\":graphemes,\"mode\":mode})\n",
    "data[\"word\"]=data[\"graphemes\"].progress_apply(lambda x:\"\".join(x))\n",
    "data=data[[\"filename\",\"word\",\"graphemes\",\"mode\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "# test train split\n",
    "srcs=list(data[\"mode\"].unique())\n",
    "random.shuffle(srcs)\n",
    "eval_len=int(len(srcs)*split_test/100)\n",
    "eval_srcs=srcs[:eval_len]\n",
    "data[\"mode\"]=data[\"mode\"].progress_apply(lambda x: \"test\" if x in eval_srcs else \"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(main_path,\"data.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INFO(f\"IMPORTANT: PATH TO USE FOR tools/process.py:{main_path}\",\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bangla",
   "language": "python",
   "name": "bangla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
