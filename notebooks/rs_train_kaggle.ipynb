{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Make sure the dataset in in the following structre\n```\n* dataset (\"/input/dataset/)\n|-dataset_iden\n |-dataset_iden\n  |-x.tfrecord\n  |-x.tfrecord\n  |-x.tfrecord\n  .................\n|-config.json\n|-enc.h5\n|-seq.h5\n|-pos.h5\n|-fuse.h5\n```","metadata":{}},{"cell_type":"markdown","source":"# Change the variables in first cell accordingly","metadata":{}},{"cell_type":"code","source":"#------------------------------\n# change able params\n#------------------------------\n#--> dataset pipeline\nPER_REPLICA_BATCH_SIZE  = 128                           # batch size per replica\nEPOCHS                  = 100                           # number of epochs to train\nDATASET_IDENTIFIER      = 'apsis-cdr-gen-bangla-final'  # kaggle dataset name\nuse_pretrained          = True                          # train from a pretrained version \neval_split              = 20                            # % of total data to use for evaluation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Every thing is fixed from this point on","metadata":{}},{"cell_type":"code","source":"import os\ninp_path=f\"../input/{DATASET_IDENTIFIER}/\"\n#--> data property\nconfig_json  =  f'{inp_path}config.json'           # @path to vocab.json\n#--> weights\n# only applicable when use_pretrained is true\nenc_weights_path        = f'{inp_path}enc.h5'  # path to \"enc.h5\"\nseq_weights_path        = f'{inp_path}seq.h5'  # path to \"seq.h5\"\npos_weights_path        = f'{inp_path}pos.h5'  # path to \"pos.h5\"\nfuse_weights_path       = f'{inp_path}fuse.h5'  # path to \"fuse.h5\"\n\nassert os.path.exists(config_json),\"config.json not found\"\nif use_pretrained:\n    assert os.path.exists(enc_weights_path ),\"enc.h5 not found\"\n    assert os.path.exists(seq_weights_path ),\"seq.h5 not found\"\n    assert os.path.exists(pos_weights_path ),\"pos.h5 not found\"\n    assert os.path.exists(fuse_weights_path ),\"fuse.h5 not found\"\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#----------------\n# imports\n#---------------\nimport tensorflow as tf\nimport random\nimport json\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom kaggle_datasets import KaggleDatasets\nimport random\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \nimport tensorflow as tf\n#-------------------\n# fixed params\n#------------------\nnb_channels =  3        \nenc_filters =  256\nfactor      =  32\n#-------------\n# config-globals\n#-------------\nwith open(config_json) as f:\n    conf = json.load(f)\n\nprint(conf)\n\nimg_height  =  conf[\"img_height\"]\nimg_width   =  conf[\"img_width\"]\nvocab       =  conf[\"vocab\"]\npos_max     =  conf[\"pos_max\"]\nRECORD_SIZE =  conf[\"tf_size\"] \nzip_iden    =  conf[\"zip_iden\"]\n\n\n# calculated\nenc_shape   =  (img_height//factor,img_width//factor, enc_filters )\nattn_shape  =  (None, enc_filters )\nmask_len    =  int((img_width//factor)*(img_height//factor))\nstart_end   =  len(vocab)+1\npad_value   =  len(vocab)+2   \n\nprint(\"Label len:\",pos_max)\nprint(\"Vocab len:\",len(vocab))\nprint(\"Start End:\",start_end)\nprint(\"pad_value:\",pad_value)\n\n\n\n#--------------------------\n# GCS Paths and tfrecords\n#-------------------------\ndef get_tfrecs(tfrec_folder_path):\n    gcs_pattern=os.path.join(tfrec_folder_path,'*.tfrecord')\n    file_paths = tf.io.gfile.glob(gcs_pattern)\n    random.shuffle(file_paths)\n    print(f\"{tfrec_folder_path}:\",len(file_paths))\n    return file_paths\n\n\n\n\nGCS_PATH = KaggleDatasets().get_gcs_path(DATASET_IDENTIFIER)\nrec_path=os.path.join(GCS_PATH,zip_iden,zip_iden) \nrecs=get_tfrecs(rec_path)\n# dist/split\nlen_recs=len(recs)\nnb_eval_recs=int(len_recs*(eval_split/100))\n\neval_recs =recs[:nb_eval_recs]\ntrain_recs=recs[nb_eval_recs:]\nrandom.shuffle(eval_recs)\nrandom.shuffle(train_recs)\n\nprint(\"Eval-recs:\",len(eval_recs))\nprint(\"Train-recs:\",len(train_recs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#----------------------------------------------------------\n# Detect hardware, return appropriate distribution strategy\n#----------------------------------------------------------\n# TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    tf.config.optimizer.set_jit(True)\nelse:\n    strategy = tf.distribute.get_strategy() \n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-------------------------------------\n# batching , strategy and steps\n#-------------------------------------\nif strategy.num_replicas_in_sync==1:\n    BATCH_SIZE = PER_REPLICA_BATCH_SIZE\nelse:\n    BATCH_SIZE = PER_REPLICA_BATCH_SIZE*strategy.num_replicas_in_sync\n\n# set    \nSTEPS_PER_EPOCH = (len(train_recs)*RECORD_SIZE)//BATCH_SIZE\nEVAL_STEPS      = (len(eval_recs)*RECORD_SIZE)//BATCH_SIZE\nprint(\"Steps:\",STEPS_PER_EPOCH)\nprint(\"Batch Size:\",BATCH_SIZE)\nprint(\"Eval Steps:\",EVAL_STEPS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n# parsing tfrecords basic\n#------------------------------\ndef data_input_fn(recs,mode): \n    '''\n      This Function generates data from gcs\n      * The parser function should look similiar now because of datasetEDA\n    '''\n    def _parser(example):   \n        feature ={  'image'  : tf.io.FixedLenFeature([],tf.string) ,\n                    'label'  : tf.io.FixedLenFeature([pos_max],tf.int64),\n                    'mask'   : tf.io.FixedLenFeature([mask_len],tf.int64)\n        }    \n        parsed_example=tf.io.parse_single_example(example,feature)\n        # image\n        image_raw=parsed_example['image']\n        image=tf.image.decode_png(image_raw,channels=nb_channels)\n        image=tf.cast(image,tf.float32)/255.0\n        image=tf.reshape(image,(img_height,img_width,nb_channels))\n        \n        # label\n        label=parsed_example['label']\n            \n        # position\n        pos=tf.range(0,pos_max)\n        pos=tf.cast(pos,tf.int32)\n        # mask\n        mask=parsed_example['mask']\n        mask=1-tf.cast(mask,tf.float32)\n        mask=tf.stack([mask for _ in range(pos_max)])\n\n        return {\"image\":image,\"label\":tf.cast(label, tf.int32),\"pos\":pos,\"mask\":mask},tf.cast(label, tf.float32)\n    \n      \n\n    # fixed code (for almost all tfrec training)\n    dataset = tf.data.TFRecordDataset(recs)\n    dataset = dataset.map(_parser)\n    dataset = dataset.shuffle(2048,reshuffle_each_iteration=True)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n    return dataset\n\ntrain_ds  =   data_input_fn(train_recs,\"train\")\neval_ds  =   data_input_fn(eval_recs,\"eval\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------\n# visualizing data\n#------------------------\n\n\nprint(\"---------------------------------------------------------------\")\nprint(\"visualizing data\")\nprint(\"---------------------------------------------------------------\")\nfor x,y in train_ds.take(1):\n    data=np.squeeze(x[\"image\"][0])\n    plt.imshow(data)\n    plt.show()\n    print(\"---------------------------------------------------------------\")\n    print(\"label:\",x[\"label\"][0])\n    print(\"---------------------------------------------------------------\")\n    print(\"pos:\",x[\"pos\"][0])\n    print(\"---------------------------------------------------------------\")\n    print(\"mask:\",x[\"mask\"][0][0])\n    print(\"---------------------------------------------------------------\")\n    print('Image Batch Shape:',x[\"image\"].shape)\n    print('Label Batch Shape:',x[\"label\"].shape)\n    print('Position Batch Shape:',x[\"pos\"].shape)\n    print('Mask Batch Shape:',x[\"mask\"].shape)\n    print(\"---------------------------------------------------------------\")\n    print('Target Batch Shape:',y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------------------------------\n#creating Embedding Weights\n#-----------------------------------\nif not use_pretrained:\n    import torch\n    import torch.nn as nn\n    seq_emb              = nn.Embedding(pad_value+2,enc_filters, padding_idx=pad_value)\n    seq_emb_weight       = seq_emb.weight.data.numpy()\n    print(seq_emb_weight.shape)\n    pos_emb              = nn.Embedding(pos_max+1,enc_filters)\n    pos_emb_weight       = pos_emb.weight.data.numpy()\n    print(pos_emb_weight.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DotAttention(tf.keras.layers.Layer):\n    \"\"\"\n        Calculate the attention weights.\n        q, k, v must have matching leading dimensions.\n        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n        The mask has different shapes depending on its type(padding or look ahead)\n        but it must be broadcastable for addition.\n\n        Args:\n        q: query shape == (..., seq_len_q, depth)\n        k: key shape == (..., seq_len_k, depth)\n        v: value shape == (..., seq_len_v, depth_v)\n        mask: Float tensor with shape broadcastable\n              to (..., seq_len_q, seq_len_k). Defaults to None.\n\n        Returns:\n        output\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.inf_val=-1e9\n        \n    def call(self,q, k, v, mask):\n        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n       \n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += (mask * self.inf_val)\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n        # add up to 1.\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \n\n\ndef encoder():\n    '''\n    creates the encoder part:\n    * defatult backbone : DenseNet121 **changeable\n    args:\n      img           : input image layer\n        \n    returns:\n      enc           : channel reduced feature layer\n\n    '''\n    # img input\n    img=tf.keras.Input(shape=(img_height,img_width,nb_channels),name='image')\n    # backbone\n    backbone=tf.keras.applications.DenseNet121(input_tensor=img ,weights=None,include_top=False)\n    # feat_out\n    enc=backbone.output\n    # enc \n    enc=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(enc)\n    return tf.keras.Model(inputs=img,outputs=enc,name=\"rs_encoder\")\n\ndef seq_decoder():\n    '''\n    sequence attention decoder (for training)\n    Tensorflow implementation of : \n    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/sequence_attention_decoder.py\n    '''\n    # label input\n    gt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"label\")\n    # mask\n    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n    # encoder\n    enc=tf.keras.Input(shape=enc_shape,name='enc_seq')\n    \n    # embedding,weights=[seq_emb_weight]\n    embedding=tf.keras.layers.Embedding(pad_value+2,enc_filters)(gt)\n    # sequence layer (2xlstm)\n    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(embedding)\n    query=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n    # attention modeling\n    # value\n    bs,h,w,nc=enc.shape\n    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n    attn=DotAttention()(query,value,value,mask)\n    return tf.keras.Model(inputs=[gt,enc,mask],outputs=attn,name=\"rs_seq_decoder\")\n \n\n\ndef pos_decoder():\n    '''\n    position attention decoder (for training)\n    Tensorflow implementation of : \n    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/position_attention_decoder.py\n    '''\n    # pos input\n    pt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"pos\")\n    # mask\n    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n    # encoder\n    enc=tf.keras.Input(shape=enc_shape,name='enc_pos')\n    \n    # embedding,weights=[pos_emb_weight]\n    query=tf.keras.layers.Embedding(pos_max+1,enc_filters)(pt)\n    # part-1:position_aware_module\n    bs,h,w,nc=enc.shape\n    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n    # sequence layer (2xlstm)\n    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(value)\n    x=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n    x=tf.keras.layers.Reshape((h,w,nc))(x)\n    # mixer\n    x=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n    x=tf.keras.layers.Activation(\"relu\")(x)\n    key=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n    bs,h,w,c=key.shape\n    key=tf.keras.layers.Reshape((h*w,nc))(key)\n    attn=DotAttention()(query,key,value,mask)\n    return tf.keras.Model(inputs=[pt,enc,mask],outputs=attn,name=\"rs_pos_decoder\")\n\ndef fusion():\n    '''\n    fuse the output of gt_attn and pt_attn \n    '''\n    # label input\n    gt_attn=tf.keras.Input(shape=attn_shape,name=\"gt_attn\")\n    # pos input\n    pt_attn=tf.keras.Input(shape=attn_shape,name=\"pt_attn\")\n    \n    x=tf.keras.layers.Concatenate()([gt_attn,pt_attn])\n    # Linear\n    x=tf.keras.layers.Dense(enc_filters*2,activation=None)(x)\n    # GLU\n    xl=tf.keras.layers.Activation(\"linear\")(x)\n    xs=tf.keras.layers.Activation(\"sigmoid\")(x)\n    x =tf.keras.layers.Multiply()([xl,xs])\n    # prediction\n    x=tf.keras.layers.Dense(pad_value+1,activation=None)(x)\n    return tf.keras.Model(inputs=[gt_attn,pt_attn],outputs=x,name=\"rs_fusion\")\n\nwith strategy.scope():\n    rs_encoder    =  encoder()\n    rs_seq_decoder=  seq_decoder()\n    rs_pos_decoder=  pos_decoder()\n    rs_fusion     =  fusion()\n    if use_pretrained:\n        rs_encoder.load_weights(enc_weights_path)\n        rs_seq_decoder.load_weights(seq_weights_path)\n        rs_pos_decoder.load_weights(pos_weights_path)\n        rs_fusion.load_weights(fuse_weights_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # optimizer\n    optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n    # loss\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    def CE_loss(real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, pad_value))\n        loss_ = loss_object(real, pred)\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n    def C_acc(real, pred):\n        accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n        mask = tf.math.logical_not(tf.math.greater_equal(real, start_end))\n        accuracies = tf.math.logical_and(mask, accuracies)\n        accuracies = tf.cast(accuracies, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class robust_scanner(tf.keras.Model):\n    def __init__(self,encoder,seq_decoder,pos_decoder,fusion):\n        super(robust_scanner, self).__init__()\n        self.encoder     = encoder\n        self.seq_decoder = seq_decoder\n        self.pos_decoder = pos_decoder\n        self.fusion      = fusion\n        \n    def compile(self,optimizer,loss_fn,acc):\n        super(robust_scanner, self).compile()\n        self.optimizer = optimizer\n        self.loss_fn   = loss_fn\n        self.acc       = acc\n       \n        \n    def train_step(self, batch_data):\n        data,gt= batch_data\n        image=data[\"image\"]\n        pos  =data[\"pos\"]\n        mask =data[\"mask\"]\n        # label\n        label=tf.ones_like(gt,dtype=tf.float32)*start_end\n        preds=[]\n        \n        with tf.GradientTape() as enc_tape, tf.GradientTape() as pos_dec_tape,tf.GradientTape() as seq_dec_tape,tf.GradientTape() as fusion_tape:\n            enc    = self.encoder(image, training=True)\n            pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=True)\n            \n            gt_attn= self.seq_decoder({\"label\":gt,\"enc_seq\":enc,\"mask\":mask},training=True)\n            pred   = self.fusion({\"gt_attn\":gt_attn,\"pt_attn\":pt_attn},training=True)\n            \n            # loss\n            loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n            # c acc\n            char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n            \n        # calc gradients    \n        enc_grads     = enc_tape.gradient(loss,self.encoder.trainable_variables)\n        pos_dec_grads = pos_dec_tape.gradient(loss,self.pos_decoder.trainable_variables)\n        seq_dec_grads = seq_dec_tape.gradient(loss,self.seq_decoder.trainable_variables)\n        fusion_grads  = fusion_tape.gradient(loss,self.fusion.trainable_variables)\n        \n        # apply\n        self.optimizer.apply_gradients(zip(enc_grads,self.encoder.trainable_variables))\n        self.optimizer.apply_gradients(zip(pos_dec_grads,self.pos_decoder.trainable_variables))\n\n        self.optimizer.apply_gradients(zip(seq_dec_grads,self.seq_decoder.trainable_variables))\n        self.optimizer.apply_gradients(zip(fusion_grads,self.fusion.trainable_variables))\n\n        \n        return {\"loss\"    : loss,\n                \"char_acc\": char_acc}\n    \n    def test_step(self, batch_data):\n        data,gt= batch_data\n        image=data[\"image\"]\n        pos  =data[\"pos\"]\n        mask =data[\"mask\"]\n        # label\n        label=tf.ones_like(gt,dtype=tf.float32)*start_end\n        preds=[]\n        \n        enc    = self.encoder(image, training=False)\n        pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=False)\n        \n        for i in range(pos_max):\n            gt_attn=self.seq_decoder({\"label\":label,\"enc_seq\":enc,\"mask\":mask},training=False)\n            step_gt_attn=gt_attn[:,i,:]\n            step_pt_attn=pt_attn[:,i,:]\n            pred=self.fusion({\"gt_attn\":step_gt_attn,\"pt_attn\":step_pt_attn},training=False)\n            preds.append(pred)\n            # can change on error\n            char_out=tf.nn.softmax(pred,axis=-1)\n            max_idx =tf.math.argmax(char_out,axis=-1)\n            if i < pos_max - 1:\n                label=tf.unstack(label,axis=-1)\n                label[i+1]=tf.cast(max_idx,tf.float32)\n                label=tf.stack(label,axis=-1)\n                \n        pred=tf.stack(preds,axis=1)\n        # loss\n        loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n        # c acc\n        char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n        \n        \n        return {\"loss\"    : loss,\n                \"char_acc\": char_acc}\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = robust_scanner(rs_encoder,\n                           rs_seq_decoder,\n                           rs_pos_decoder,\n                           rs_fusion)\n\n    model.compile(optimizer = optimizer,\n                  loss_fn   = CE_loss,\n                  acc       = C_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduces learning rate on plateau\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,\n                                                  cooldown= 10,\n                                                  patience=3,\n                                                  verbose =1,\n                                                  min_lr=0.1e-7)\n# early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=15, \n                                                  verbose=1, \n                                                  mode = 'auto') \n\n\nclass SaveBestModel(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.best = float('inf')\n\n    def on_epoch_end(self, epoch, logs=None):\n        metric_value = logs['val_loss']\n        if metric_value < self.best:\n            print(f\"Loss Improved epoch:{epoch} from {self.best} to {metric_value}\")\n            self.best = metric_value\n            self.model.encoder.save_weights(\"enc.h5\")\n            self.model.seq_decoder.save_weights(\"seq.h5\")\n            self.model.pos_decoder.save_weights(\"pos.h5\")\n            self.model.fusion.save_weights(f\"fuse.h5\")\n            print(\"Saved Best Weights\")\n    def set_model(self, model):\n        self.model = model\n            \nmodel_save=SaveBestModel()\nmodel_save.set_model(model)\ncallbacks= [lr_reducer,early_stopping,model_save]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_ds,\n                  epochs=EPOCHS,\n                  steps_per_epoch=STEPS_PER_EPOCH,\n                  verbose=1,\n                  validation_data=eval_ds,\n                  validation_steps=EVAL_STEPS, \n                  callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}